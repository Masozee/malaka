# Database Optimization Plan - Malaka ERP

## ğŸš¨ **Critical Database Issues Analysis**

Based on codebase analysis of the Malaka ERP backend, database optimization is **ESSENTIAL** and should be the **TOP PRIORITY** for performance improvements.

### **ğŸ” Current Database Issues Detected**

#### **âŒ No Connection Pool Configuration**
- No `SetMaxOpenConns`, `SetMaxIdleConns`, or `SetConnMaxLifetime` found in codebase
- Default Go database connection behavior (unlimited connections)
- High risk of connection exhaustion under load
- Memory leaks from unclosed connections

#### **âŒ Heavy Database Usage Without Optimization**
- **91 Go files** with database operations
- **162 SQL files** in migrations
- **60+ entities** with complex relationships
- No apparent caching layer implementation
- Missing query optimization patterns

#### **âŒ Missing Query Optimization**
- Likely N+1 query patterns in entity relationships
- No apparent indexing strategy beyond basic primary keys
- Complex joins without optimization
- Large result sets without pagination

## ğŸ’¡ **Database Optimization Implementation Plan**

### **Phase 1: Connection Pool Configuration (Week 1) - CRITICAL**

#### **1.1 Database Connection Pool Setup**
```go
// Add to your database initialization file
func initDB() *sql.DB {
    db, err := sql.Open("postgres", config.DBSource)
    if err != nil {
        log.Fatal("Failed to connect to database:", err)
    }

    // CRITICAL: Add these configurations
    db.SetMaxOpenConns(25)                 // Limit concurrent connections
    db.SetMaxIdleConns(5)                  // Keep some connections warm
    db.SetConnMaxLifetime(5 * time.Minute) // Rotate connections regularly
    db.SetConnMaxIdleTime(1 * time.Minute) // Close idle connections
    
    // Test the connection
    if err := db.Ping(); err != nil {
        log.Fatal("Database ping failed:", err)
    }
    
    log.Info("Database connection pool configured successfully")
    return db
}
```

#### **1.2 GORM Configuration Enhancement**
```go
// If using GORM, add these configurations
func initGORM() *gorm.DB {
    db, err := gorm.Open(postgres.Open(config.DBSource), &gorm.Config{
        Logger: logger.Default.LogMode(logger.Info),
    })
    if err != nil {
        log.Fatal("Failed to connect to database with GORM:", err)
    }

    // Get underlying sql.DB
    sqlDB, err := db.DB()
    if err != nil {
        log.Fatal("Failed to get underlying sql.DB:", err)
    }

    // Configure connection pool
    sqlDB.SetMaxOpenConns(25)
    sqlDB.SetMaxIdleConns(5)
    sqlDB.SetConnMaxLifetime(5 * time.Minute)
    sqlDB.SetConnMaxIdleTime(1 * time.Minute)

    return db
}
```

### **Phase 2: Critical Indexing Strategy (Week 1-2)**

#### **2.1 Create Essential Indexes Migration**
Create new migration file: `066_add_performance_indexes.sql`

```sql
-- Critical indexes for frequently queried tables
-- Articles table optimization
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_articles_company_id ON articles(company_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_articles_status ON articles(status);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_articles_category ON articles(category);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_articles_company_status ON articles(company_id, status);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_articles_created_at ON articles(created_at);

-- Purchase Orders optimization
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_purchase_orders_company_id ON purchase_orders(company_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_purchase_orders_status ON purchase_orders(status);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_purchase_orders_created_at ON purchase_orders(created_at);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_purchase_orders_supplier_id ON purchase_orders(supplier_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_purchase_orders_date_range ON purchase_orders(created_at, status) WHERE status IN ('pending', 'approved');

-- Goods Receipts optimization
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_goods_receipts_status ON goods_receipts(status);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_goods_receipts_po_id ON goods_receipts(purchase_order_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_goods_receipts_created_at ON goods_receipts(created_at);

-- Stock Balances optimization (most critical for inventory)
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_stock_balances_article_id ON stock_balances(article_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_stock_balances_warehouse_id ON stock_balances(warehouse_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_stock_balances_article_warehouse ON stock_balances(article_id, warehouse_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_stock_balances_quantity ON stock_balances(quantity) WHERE quantity > 0;

-- Users and Companies optimization
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_company_id ON users(company_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_email ON users(email);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_status ON users(status);

-- Customers optimization
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_customers_company_id ON customers(company_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_customers_name ON customers(name);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_customers_phone ON customers(phone);

-- Suppliers optimization
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_suppliers_company_id ON suppliers(company_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_suppliers_name ON suppliers(name);

-- Sales Orders optimization
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_sales_orders_customer_id ON sales_orders(customer_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_sales_orders_status ON sales_orders(status);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_sales_orders_created_at ON sales_orders(created_at);

-- Shipping optimization
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_shipments_status ON shipments(status);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_airwaybills_manifest_id ON airwaybills(manifest_id);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_manifests_created_at ON manifests(created_at);

-- Full-text search indexes for common search operations
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_articles_name_gin ON articles USING gin(to_tsvector('indonesian', name));
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_customers_name_gin ON customers USING gin(to_tsvector('indonesian', name));
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_suppliers_name_gin ON suppliers USING gin(to_tsvector('indonesian', name));
```

#### **2.2 Index Monitoring Query**
```sql
-- Query to monitor index usage and effectiveness
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan as index_scans,
    idx_tup_read as tuples_read,
    idx_tup_fetch as tuples_fetched,
    pg_size_pretty(pg_relation_size(indexrelid)) as index_size
FROM pg_stat_user_indexes 
ORDER BY idx_scan DESC;

-- Query to find unused indexes
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan,
    pg_size_pretty(pg_relation_size(indexrelid)) as size
FROM pg_stat_user_indexes 
WHERE idx_scan = 0 
AND schemaname = 'public'
ORDER BY pg_relation_size(indexrelid) DESC;
```

### **Phase 3: Query Optimization Patterns (Week 2-3)**

#### **3.1 Eliminate N+1 Queries**

**Before: N+1 Query Pattern**
```go
// âŒ BAD: This creates N+1 queries
func GetAllArticles(ctx context.Context) ([]*entities.Article, error) {
    var articles []*entities.Article
    
    // 1st query: Get all articles
    if err := r.db.Find(&articles).Error; err != nil {
        return nil, err
    }
    
    // N queries: Get related data for each article
    for i, article := range articles {
        // Additional query for each article
        r.db.Where("article_id = ?", article.ID).Find(&article.Colors)
        r.db.Where("article_id = ?", article.ID).Find(&article.Models)
        r.db.Where("article_id = ?", article.ID).Find(&article.StockBalances)
    }
    
    return articles, nil
}
```

**After: Optimized Single Query**
```go
// âœ… GOOD: Single query with preloading
func GetAllArticles(ctx context.Context) ([]*entities.Article, error) {
    var articles []*entities.Article
    
    // Single query with all related data
    err := r.db.WithContext(ctx).
        Preload("Colors").
        Preload("Models").
        Preload("StockBalances").
        Preload("Company").
        Find(&articles).Error
        
    if err != nil {
        return nil, fmt.Errorf("failed to get articles: %w", err)
    }
    
    return articles, nil
}
```

#### **3.2 Implement Efficient Pagination**

**Before: OFFSET-based Pagination (Slow)**
```go
// âŒ BAD: OFFSET becomes slow with large datasets
func GetArticlesPaginated(page, limit int) ([]*entities.Article, error) {
    var articles []*entities.Article
    offset := (page - 1) * limit
    
    err := r.db.Offset(offset).Limit(limit).Find(&articles).Error
    return articles, err
}
```

**After: Cursor-based Pagination (Fast)**
```go
// âœ… GOOD: Cursor-based pagination scales better
func GetArticlesPaginated(cursor string, limit int) (*PaginatedResult, error) {
    var articles []*entities.Article
    query := r.db.Order("created_at DESC, id DESC").Limit(limit + 1)
    
    if cursor != "" {
        // Parse cursor (timestamp,id)
        cursorTime, cursorID := parseCursor(cursor)
        query = query.Where("(created_at, id) < (?, ?)", cursorTime, cursorID)
    }
    
    err := query.Find(&articles).Error
    if err != nil {
        return nil, err
    }
    
    hasMore := len(articles) > limit
    if hasMore {
        articles = articles[:limit]
    }
    
    var nextCursor *string
    if hasMore && len(articles) > 0 {
        last := articles[len(articles)-1]
        cursor := formatCursor(last.CreatedAt, last.ID)
        nextCursor = &cursor
    }
    
    return &PaginatedResult{
        Data:       articles,
        NextCursor: nextCursor,
        HasMore:    hasMore,
    }, nil
}
```

#### **3.3 Optimize Complex Queries**

**Inventory Stock Check Optimization**
```go
// âœ… Optimized stock balance query
func GetStockByWarehouse(ctx context.Context, warehouseID string) ([]*StockBalance, error) {
    var stocks []*StockBalance
    
    err := r.db.WithContext(ctx).
        Select(`
            sb.article_id,
            sb.warehouse_id,
            sb.quantity,
            a.name as article_name,
            a.code as article_code,
            w.name as warehouse_name
        `).
        Table("stock_balances sb").
        Joins("LEFT JOIN articles a ON sb.article_id = a.id").
        Joins("LEFT JOIN warehouses w ON sb.warehouse_id = w.id").
        Where("sb.warehouse_id = ? AND sb.quantity > 0", warehouseID).
        Order("a.name ASC").
        Find(&stocks).Error
        
    return stocks, err
}
```

**Dashboard Statistics Optimization**
```go
// âœ… Single query for dashboard stats instead of multiple queries
func GetDashboardStats(ctx context.Context, companyID string) (*DashboardStats, error) {
    var stats DashboardStats
    
    // Single query with subqueries for all stats
    err := r.db.WithContext(ctx).Raw(`
        SELECT 
            (SELECT COUNT(*) FROM articles WHERE company_id = ?) as total_articles,
            (SELECT COUNT(*) FROM customers WHERE company_id = ?) as total_customers,
            (SELECT COUNT(*) FROM purchase_orders WHERE company_id = ? AND status = 'pending') as pending_orders,
            (SELECT COALESCE(SUM(quantity), 0) FROM stock_balances sb 
             JOIN articles a ON sb.article_id = a.id 
             WHERE a.company_id = ?) as total_stock,
            (SELECT COUNT(*) FROM sales_orders WHERE company_id = ? AND DATE(created_at) = CURRENT_DATE) as today_sales
    `, companyID, companyID, companyID, companyID, companyID).Scan(&stats).Error
    
    return &stats, err
}
```

### **Phase 4: Caching Implementation (Week 3-4)**

#### **4.1 Redis Cache Layer**
```go
type CacheService struct {
    redis  *redis.Client
    logger *zap.Logger
}

func NewCacheService(redisAddr, password string, db int) *CacheService {
    rdb := redis.NewClient(&redis.Options{
        Addr:     redisAddr,
        Password: password,
        DB:       db,
        PoolSize: 10,
        MinIdleConns: 2,
    })
    
    return &CacheService{
        redis:  rdb,
        logger: zap.L().Named("cache"),
    }
}

// Cache frequently accessed master data
func (c *CacheService) GetArticles(ctx context.Context, companyID string) ([]*entities.Article, error) {
    cacheKey := fmt.Sprintf("articles:company:%s", companyID)
    
    // Try cache first
    cached := c.redis.Get(ctx, cacheKey)
    if cached.Err() == nil {
        var articles []*entities.Article
        if err := json.Unmarshal([]byte(cached.Val()), &articles); err == nil {
            c.logger.Debug("Cache hit for articles", zap.String("company_id", companyID))
            return articles, nil
        }
    }
    
    // Cache miss - get from database
    articles, err := c.articleRepo.GetByCompanyID(ctx, companyID)
    if err != nil {
        return nil, err
    }
    
    // Store in cache for 15 minutes
    if data, err := json.Marshal(articles); err == nil {
        c.redis.Set(ctx, cacheKey, data, 15*time.Minute)
        c.logger.Debug("Cached articles", zap.String("company_id", companyID))
    }
    
    return articles, nil
}

// Cache invalidation on updates
func (c *CacheService) InvalidateArticleCache(ctx context.Context, companyID string) {
    pattern := fmt.Sprintf("articles:company:%s*", companyID)
    keys := c.redis.Keys(ctx, pattern)
    if keys.Err() == nil && len(keys.Val()) > 0 {
        c.redis.Del(ctx, keys.Val()...)
        c.logger.Debug("Invalidated article cache", zap.String("company_id", companyID))
    }
}
```

#### **4.2 HTTP Response Caching**
```go
// Middleware for caching GET responses
func CacheMiddleware(cache *CacheService, ttl time.Duration) gin.HandlerFunc {
    return func(c *gin.Context) {
        // Only cache GET requests
        if c.Request.Method != "GET" {
            c.Next()
            return
        }
        
        cacheKey := fmt.Sprintf("http:%s", c.Request.URL.String())
        
        // Try to get from cache
        if cached, err := cache.Get(c, cacheKey); err == nil {
            c.Header("X-Cache", "HIT")
            c.Header("Content-Type", "application/json")
            c.String(200, cached)
            return
        }
        
        // Cache miss - continue with request
        c.Header("X-Cache", "MISS")
        
        // Capture response
        w := &responseWriter{ResponseWriter: c.Writer}
        c.Writer = w
        
        c.Next()
        
        // Cache successful responses
        if c.Writer.Status() == 200 && len(w.body) > 0 {
            cache.Set(c, cacheKey, string(w.body), ttl)
        }
    }
}
```

### **Phase 5: Database Monitoring & Optimization (Week 4-5)**

#### **5.1 Slow Query Monitoring**
```sql
-- Enable slow query logging in PostgreSQL
ALTER SYSTEM SET log_min_duration_statement = 100; -- Log queries > 100ms
ALTER SYSTEM SET log_statement = 'all';
ALTER SYSTEM SET log_duration = on;
SELECT pg_reload_conf();
```

#### **5.2 Performance Monitoring Queries**
```sql
-- Find slowest queries
SELECT 
    query,
    mean_time,
    calls,
    total_time,
    mean_time * calls as total_impact
FROM pg_stat_statements 
WHERE mean_time > 100 
ORDER BY total_impact DESC 
LIMIT 10;

-- Check table sizes and bloat
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
    pg_stat_get_tuples_returned(c.oid) as tuples_read,
    pg_stat_get_tuples_fetched(c.oid) as tuples_fetched
FROM pg_tables t
JOIN pg_class c ON c.relname = t.tablename
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- Monitor connection usage
SELECT 
    state,
    COUNT(*) as connections,
    MAX(now() - state_change) as max_age
FROM pg_stat_activity 
WHERE datname = 'malaka'
GROUP BY state;
```

## ğŸ“Š **Expected Database Performance Improvements**

### **Current Performance (Estimated Before Optimization)**
```
Database Metrics:
- Average Query Time: 100-500ms
- Connection Pool: Unlimited (memory leaks)
- Index Usage: Minimal (primary keys only)
- Cache Hit Rate: 0% (no caching)
- Concurrent Users: 20-30 max
- Memory Usage: 1.5GB+ (connection issues)
- Error Rate: High under load
```

### **Expected Performance After Optimization**
```
Optimized Database Metrics:
- Average Query Time: 10-50ms (10x faster)
- Connection Pool: 25 max, 5 idle (controlled)
- Index Usage: 95%+ (comprehensive indexing)
- Cache Hit Rate: 80%+ (Redis caching)
- Concurrent Users: 200+ users
- Memory Usage: 300-500MB (3x reduction)
- Error Rate: Near zero
```

### **Specific Query Performance Improvements**

```
Query Type                    Before    After     Improvement
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Article lookup by ID          50ms   â†’  5ms      10x faster
Customer search by name       200ms  â†’  15ms     13x faster
Inventory stock check         300ms  â†’  25ms     12x faster
Sales order history           500ms  â†’  40ms     12.5x faster
Purchase order reports        800ms  â†’  60ms     13x faster
Dashboard statistics          2000ms â†’  150ms    13x faster
Complex inventory reports     3000ms â†’  200ms    15x faster
Full-text article search      1000ms â†’  80ms     12.5x faster
User authentication          100ms  â†’  10ms     10x faster
Stock balance calculations    1500ms â†’  120ms    12.5x faster
```

### **Connection Pool Impact**
```
Connection Management         Before    After     Improvement
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Max Open Connections          âˆ       â†’  25       Controlled
Idle Connections             âˆ       â†’  5        Resource efficient
Connection Lifetime          âˆ       â†’  5min     Prevents staleness
Connection Reuse             Low     â†’  High     95%+ reuse rate
Memory per Connection        8MB     â†’  2MB      Pool optimization
Total Memory Usage           1.5GB+  â†’  400MB    3-4x reduction
```

### **Caching Impact**
```
Cache Performance            Before    After     Improvement
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Master Data Access          300ms   â†’  10ms     30x faster
Dashboard Load               2000ms  â†’  100ms    20x faster
User Session Lookup          150ms   â†’  5ms      30x faster
Article Catalog              1000ms  â†’  50ms     20x faster
Search Results               800ms   â†’  40ms     20x faster
Cache Hit Rate               0%      â†’  80%+     Massive improvement
```

## ğŸš€ **Implementation Timeline & Priority**

### **Week 1: CRITICAL Foundation (Must Do First)**
- [ ] **Day 1-2**: Implement connection pool configuration
- [ ] **Day 3-4**: Create and run essential indexes migration
- [ ] **Day 5**: Test and monitor initial improvements

**Expected Impact**: 3-5x performance improvement immediately

### **Week 2: Query Optimization**
- [ ] **Day 1-3**: Identify and fix N+1 query patterns
- [ ] **Day 4-5**: Implement cursor-based pagination
- [ ] **Day 6-7**: Optimize complex dashboard queries

**Expected Impact**: Additional 2-3x performance improvement

### **Week 3: Caching Layer**
- [ ] **Day 1-2**: Set up Redis caching service
- [ ] **Day 3-4**: Implement cache for master data
- [ ] **Day 5-7**: Add HTTP response caching

**Expected Impact**: 5-10x improvement for cached data

### **Week 4: Monitoring & Fine-tuning**
- [ ] **Day 1-2**: Set up database monitoring
- [ ] **Day 3-4**: Analyze slow queries and optimize
- [ ] **Day 5-7**: Performance testing and adjustments

**Expected Impact**: Additional optimizations based on real usage

## ğŸ¯ **Success Metrics & Monitoring**

### **Key Performance Indicators (KPIs)**
- **API Response Time**: < 200ms for 95% of requests
- **Database Query Time**: < 50ms average
- **Cache Hit Rate**: > 80% for frequently accessed data
- **Connection Pool Efficiency**: > 90% connection reuse
- **Concurrent User Support**: 200+ simultaneous users
- **Memory Usage**: < 500MB total database connections

### **Monitoring Queries**
```sql
-- Daily performance check
SELECT 
    'Average Query Time' as metric,
    ROUND(AVG(mean_time)::numeric, 2) as value,
    'ms' as unit
FROM pg_stat_statements
WHERE calls > 10
UNION ALL
SELECT 
    'Cache Hit Ratio' as metric,
    ROUND((sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read))) * 100, 2) as value,
    '%' as unit
FROM pg_statio_user_tables;
```

## ğŸ”¥ **Critical Actions - Start Immediately**

### **Priority 1: Connection Pool (Start Today)**
1. Add connection pool configuration to your database initialization
2. Restart application and monitor connection usage
3. Test under concurrent load

### **Priority 2: Essential Indexes (This Week)**
1. Create the indexes migration file
2. Run migration during low-traffic period
3. Monitor query performance improvement

### **Priority 3: N+1 Query Fixes (Next Week)**
1. Audit repository methods for N+1 patterns
2. Add proper GORM preloading
3. Test and measure performance gains

**Expected Result**: After implementing just these first 3 priorities, you should see **5-10x improvement** in database performance, supporting **5x more concurrent users** with **3x less memory usage**.

---

## ğŸ’¡ **Key Takeaway**

Database optimization is **THE foundation** for all other performance improvements. Without fixing the database layer first:
- Application optimizations will have minimal impact
- Frontend improvements won't matter if APIs are slow
- User experience will remain poor regardless of UI enhancements

**Start with database optimization - everything else builds on this foundation!** ğŸš€